{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e927e55",
   "metadata": {},
   "source": [
    "## Sentiment analysis with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90fd7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1ec90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3192d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "data = pd.read_csv(\"d:\\\\Temp\\\\training.1600000.processed.noemoticon.csv\", \n",
    "                   header=None, names=cols, engine=\"python\", encoding=\"latin1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e312e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c369f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d996463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text() # clear LXML\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet) # clear all refs\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?]\", ' ', tweet)\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca4e5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ffe4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8432ff",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "155debcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\",\n",
    "                            trainable=False)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1de49393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "93f1cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = [encode_sentence(sent) for sent in data_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f140083e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'dog', 'loves', 'straw', '##berries', '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"My dog loves strawberries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "085a3ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2026, 3899, 7459, 13137, 20968, 1012]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"My dog loves strawberries.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cdcd8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4f5e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = [encode_sentence(s) for s in data_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1473d6",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0f0d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i], len(sent)] for i, sent in enumerate(data_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b24c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d403c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_len.sort(key = lambda x : x[2]) # sort by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dadfc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_all = [(sent_lab[0], sent_lab[1]) for sent_lab in data_with_len if sent_lab[2] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2adef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(tokens):\n",
    "    return tokenizer.convert_tokens_to_idskens_to_ids(tokens)\n",
    "\n",
    "def get_mask(tokens):\n",
    "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
    "\n",
    "def get_segments(tokens):\n",
    "    seg_ids = []\n",
    "    current_seg_id = 0\n",
    "    for t in tokens:\n",
    "        seg_ids.append(current_seg_id)\n",
    "        if tok == \"[SEP]\":\n",
    "            current_current_seg_idid = 1-current_seg_id\n",
    "    return seg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbc0a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ac98363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=int32, numpy=array([2339, 2515, 2002, 2191, 2033, 2061, 3407, 1029])>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=1>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(all_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c95f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "437a7010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 8), dtype=int32, numpy=\n",
       " array([[ 2339,  2515,  2002,  2191,  2033,  2061,  3407,  1029],\n",
       "        [26478,  8609,  2015,  2006,  2115, 16568,  2663,   999],\n",
       "        [ 4658,  1012,  5580,  2057,  2071,  2022,  1997,  2393],\n",
       "        [ 1053, 28765, 14841,  3211,  3347,  2059, 14175,  2252],\n",
       "        [ 2975,  2005,  2147,  2574,  1012,  1012,  1012,  1012],\n",
       "        [ 2821,  2158,  1045,  2514,  2066,  5996,  2157,  2085],\n",
       "        [ 1045,  2031,  1059, 16584, 27571,  2035,  7840,  2033],\n",
       "        [ 2383,  1037,  2645,  2007,  1996, 15041,  2919,  2335],\n",
       "        [ 2667,  2000,  8980,  1012,  2067,  2000,  2147,  4826],\n",
       "        [10166,  2008,  2015,  2428,  1012,  1012,  1012,  8680],\n",
       "        [27571,  7716,  7630,  3600,  2115, 12171,  6429, 13871],\n",
       "        [ 2067,  2013,  1996,  2009, 11360,  1012, 26304,  2009],\n",
       "        [ 2292,  2026,  2166,  3385,  2290,  6170,  2000,  2017],\n",
       "        [11082,  3246,  1052,  7274,  2097,  2393,  2149,  2574],\n",
       "        [ 2003,  3110,  2074,  2066,  2197,  2305,  2153,  1012],\n",
       "        [ 1056, 28394,  3436,  2013,  2026,  2047,  2822,  2193],\n",
       "        [ 2735,  7245, 22104,  2051,  2005,  2147,  1012,  1012],\n",
       "        [11265,  2213,  8915,  2213,  1042, 11439,  3972,  2229],\n",
       "        [ 6289, 23278, 23644, 23278,  2017,  2052,  1012,  1012],\n",
       "        [13718,  2748,  1996,  3720,  2987,  1056,  4175,  4902],\n",
       "        [ 2293,  3336,  1012,  1012,  1012,  2009,  2965,  2293],\n",
       "        [ 3582,  2033,  1998,  1045,  2222,  3582,  2017,   999],\n",
       "        [ 3712,  5051,  2987,  1056,  2147,  2006, 18059,  1012],\n",
       "        [ 2003,  3241,  2016,  1055,  2205,  5458,  2005, 19081],\n",
       "        [18758,  1045,  1049,  1999, 16392,  2127, 10424,  2072],\n",
       "        [ 2002,  5369, 22091,  2860,  5470,  5705,   999,   999],\n",
       "        [ 2984,  5754, 10360,  2024, 22079,  2050,  3102,  2033],\n",
       "        [20277,  2480,  2122, 11938,  3632,  2061,  3435,   999],\n",
       "        [15624,  1012,  2009,  2001,  1996,  4762, 10216,  1012],\n",
       "        [18168,  2290, 12532,  2003,  8966,   999,   999,   999],\n",
       "        [ 1045,  2123,  1056,  2455,  2127,  4826,  2851,  1012],\n",
       "        [ 1997, 17870,  2821,  2232,  2049,  2333,  2000,  2273]])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 1, 1, 0])>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(all_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "341f8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
    "NB_BATCHES_TEST = NB_BATCHES // 10\n",
    "all_batched.shuffle(NB_BATCHES)\n",
    "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332ae4b",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b58b4589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, emb_dim=128, nb_filters=50,\n",
    "                 FFN_units=512, nb_classes=2, dropout_rate=0.1, training=False, name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        self.embedding = layers.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        # here we are shifting filter in just 1D, as it makes no sense to do 2D for text\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding=\"valid\", activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding=\"valid\", activation=\"relu\")  \n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding=\"valid\", activation=\"relu\")  \n",
    "        \n",
    "        self.pool = layers.GlobalAveragePooling1D()\n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes, activation=\"softmax\")\n",
    "            \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3*nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8505d6a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a576a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85cc9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcnn = DCNN(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS, \n",
    "            FFN_units=FFN_UNITS, nb_classes=NB_CLASSES, dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01743ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "else:\n",
    "    dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4bd800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"ckpt_bert_tok\"\n",
    "ckpt = tf.train.Checkpoint(DCNN=dcnn)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint has been resored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9138218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpointOnEpochEndCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"\\nCheckpoint saved at {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a1284a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  36913/Unknown - 426s 11ms/step - loss: 0.4277 - accuracy: 0.8018Checkpoint saved at ckpt_bert_tok\n",
      "36913/36913 [==============================] - 427s 11ms/step - loss: 0.4277 - accuracy: 0.8018\n",
      "Epoch 2/5\n",
      "36912/36913 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.8307Checkpoint saved at ckpt_bert_tok\n",
      "36913/36913 [==============================] - 420s 11ms/step - loss: 0.3789 - accuracy: 0.8307\n",
      "Epoch 3/5\n",
      "36911/36913 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.8512Checkpoint saved at ckpt_bert_tok\n",
      "36913/36913 [==============================] - 430s 11ms/step - loss: 0.3413 - accuracy: 0.8512\n",
      "Epoch 4/5\n",
      "36913/36913 [==============================] - ETA: 0s - loss: 0.3044 - accuracy: 0.8700Checkpoint saved at ckpt_bert_tok\n",
      "36913/36913 [==============================] - 428s 11ms/step - loss: 0.3044 - accuracy: 0.8700\n",
      "Epoch 5/5\n",
      "36909/36913 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.8861Checkpoint saved at ckpt_bert_tok\n",
      "36913/36913 [==============================] - 420s 11ms/step - loss: 0.2709 - accuracy: 0.8861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b35f1961d0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcnn.fit(train_dataset, epochs=NB_EPOCHS, callbacks=[SaveCheckpointOnEpochEndCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61779eb9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e1a4bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101/4101 [==============================] - 26s 6ms/step - loss: 0.9337 - accuracy: 0.8185\n",
      "[0.93369060754776, 0.8184893727302551]\n"
     ]
    }
   ],
   "source": [
    "results = dcnn.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7de8c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sentense):\n",
    "    tokens = encode_sentence(sentense)\n",
    "    inputs = tf.expand_dims(tokens, 0)\n",
    "    output = dcnn(inputs, training=False)\n",
    "    sentiment = math.floor(output*2)\n",
    "    \n",
    "    if sentiment == 0:\n",
    "        print(\"Output of the model: {}\\nPredicted sentiment: negative\".format(output))\n",
    "    else:\n",
    "        print(\"Output of the model: {}\\nPredicted sentiment: positive\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "92f2106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the model: [[0.00115073]]\n",
      "Predicted sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"I would rather not do it again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f003744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the model: [[0.9999993]]\n",
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"This movie was pretty interesting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
